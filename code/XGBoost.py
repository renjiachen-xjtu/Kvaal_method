# 1. å®‰è£…å¿…è¦åº“ï¼ˆå¦‚æœè¿˜æ²¡å®‰è£…ï¼‰
# åœ¨ç»ˆç«¯æˆ–å‘½ä»¤è¡Œè¿è¡Œï¼ˆåªéœ€ä¸€æ¬¡ï¼‰ï¼š
# pip install pandas scikit-learn xgboost matplotlib seaborn shap

# 2. å¯¼å…¥æ‰€éœ€åº“
import pandas as pd
from sklearn.model_selection import train_test_split, GridSearchCV
from sklearn.metrics import mean_absolute_error, r2_score
from xgboost import XGBRegressor
import matplotlib.pyplot as plt
import seaborn as sns
from datetime import datetime
import shap
import numpy as np
import warnings
import os
warnings.filterwarnings('ignore')  # å¿½ç•¥æ‰€æœ‰è­¦å‘Š

# 3. è¯»å– Excel æ–‡ä»¶ï¼ˆæ‰€æœ‰å·¥ä½œè¡¨ï¼‰
excel_file = pd.read_excel(r"D:\Users\HUAWEI\Desktop\1\female.xlsx", sheet_name=None)

# === å…³é”®ä¿®æ”¹: è®¾ç½®ç»Ÿä¸€çš„è¾“å‡ºç›®å½•ç»“æ„ ===
MAIN_OUTPUT_DIR = r"D:\Users\HUAWEI\Desktop\1"  # ä¸»è¾“å‡ºç›®å½•
os.makedirs(MAIN_OUTPUT_DIR, exist_ok=True)  # ç¡®ä¿ä¸»ç›®å½•å­˜åœ¨

# æ—¥å¿—æ–‡ä»¶è·¯å¾„
output_file_path = os.path.join(MAIN_OUTPUT_DIR, "testmale.txt")

# åˆå§‹åŒ–æ—¥å¿—ï¼ˆå¸¦æ—¶é—´æˆ³ï¼‰
with open(output_file_path, 'a', encoding='utf-8') as f:
    f.write("\n" + "="*60 + "\n")
    f.write(f"ã€æ–°è¿è¡Œå¼€å§‹ã€‘æ—¶é—´: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\n")
    f.write(f"å…±æ£€æµ‹åˆ° {len(excel_file)} ä¸ªå·¥ä½œè¡¨: {list(excel_file.keys())}\n")
    f.write("="*60 + "\n")

# 4. æŒ‡å®šç‰¹å¾åˆ—
feature_columns = [
    "L=(P+R)/2",
    "M=(P+R+A+B+C)/5",
    "W=(B+C/2)",
    "W-L",
    "P=p/r",
    "R=p/t",
    "T=t/r",
    "A=a`/a",
    "B=b`/b",
    "C=c`/c"
]

# å®‰å…¨æ–‡ä»¶åå‡½æ•°ï¼ˆä¸éšæœºæ£®æ—ä»£ç ä¸€è‡´ï¼‰
def safe_filename(name):
    """ç§»é™¤æ–‡ä»¶åä¸­çš„éæ³•å­—ç¬¦"""
    return "".join(c if c.isalnum() or c in (' ', '-', '_') else '_' for c in str(name))

# === æ–°å¢ï¼šç»˜åˆ¶ç‰¹å¾é‡è¦æ€§æ¡å½¢å›¾ ===
def plot_feature_importance(model, feature_names, sheet_name, output_dir):
    """ç»˜åˆ¶ç‰¹å¾é‡è¦æ€§æ¡å½¢å›¾"""
    importances = model.feature_importances_
    indices = np.argsort(importances)[::-1]  # ä»å¤§åˆ°å°æ’åºçš„ç´¢å¼•
    
    plt.figure(figsize=(10, 6))
    plt.title(f"ç‰¹å¾é‡è¦æ€§ - {sheet_name}", fontsize=14)
    plt.bar(range(len(importances)), importances[indices], color="b", align="center")
    plt.xticks(range(len(importances)), [feature_names[i] for i in indices], rotation=45)
    plt.xlim([-1, len(importances)])
    plt.tight_layout()
    safe_name = safe_filename(sheet_name)
    plt.savefig(os.path.join(output_dir, f"ç‰¹å¾é‡è¦æ€§_{safe_name}.png"), dpi=300)
    plt.close()

# === æ–°å¢ï¼šç»˜åˆ¶è¯¯å·®åˆ†å¸ƒå›¾ ===
def plot_error_distribution(y_true, y_pred, sheet_name, output_dir):
    """ç»˜åˆ¶é¢„æµ‹è¯¯å·®åˆ†å¸ƒå›¾"""
    errors = y_true - y_pred
    
    plt.figure(figsize=(10, 6))
    sns.histplot(errors, kde=True, bins=20, color='skyblue', edgecolor='black')
    plt.axvline(x=np.mean(errors), color='r', linestyle='--', label=f'å¹³å‡è¯¯å·®: {np.mean(errors):.2f}')
    plt.xlabel("é¢„æµ‹è¯¯å·® (å®é™…å¹´é¾„ - é¢„æµ‹å¹´é¾„)", fontsize=12)
    plt.ylabel("æ ·æœ¬æ•°é‡", fontsize=12)
    plt.title(f"é¢„æµ‹è¯¯å·®åˆ†å¸ƒ - {sheet_name}", fontsize=14)
    plt.legend()
    plt.grid(alpha=0.3)
    safe_name = safe_filename(sheet_name)
    plt.savefig(os.path.join(output_dir, f"è¯¯å·®åˆ†å¸ƒ_{safe_name}.png"), dpi=300)
    plt.close()

# === æ–°å¢ï¼šç»˜åˆ¶SHAPæ‘˜è¦å›¾å’Œèœ‚ç¾¤å›¾ ===
def plot_shap_summary(model, X_sample, feature_names, sheet_name, output_dir):
    """ç»˜åˆ¶SHAPæ‘˜è¦å›¾å’Œèœ‚ç¾¤å›¾"""
    # åˆ›å»ºSHAPè§£é‡Šå™¨
    explainer = shap.TreeExplainer(model)
    shap_values = explainer.shap_values(X_sample)
    
    # SHAPæ‘˜è¦å›¾ (æ¡å½¢å›¾)
    plt.figure(figsize=(12, 8))
    shap.summary_plot(shap_values, X_sample, feature_names=feature_names, plot_type="bar", show=False)
    plt.title(f"SHAP ç‰¹å¾é‡è¦æ€§æ‘˜è¦ - {sheet_name}", fontsize=14)
    safe_name = safe_filename(sheet_name)
    plt.savefig(os.path.join(output_dir, f"SHAPæ‘˜è¦_{safe_name}.png"), dpi=300, bbox_inches='tight')
    plt.close()
    
    # SHAPèœ‚ç¾¤å›¾ (Violin Plot)
    plt.figure(figsize=(12, 8))
    shap.summary_plot(shap_values, X_sample, feature_names=feature_names, plot_type="violin", show=False)
    plt.title(f"SHAP èœ‚ç¾¤å›¾ - {sheet_name}", fontsize=14)
    plt.savefig(os.path.join(output_dir, f"SHAPèœ‚ç¾¤å›¾_{safe_name}.png"), dpi=300, bbox_inches='tight')
    plt.close()

# 5. å¯¹æ¯ä¸ªå·¥ä½œè¡¨ç‹¬ç«‹å»ºæ¨¡åˆ†æ
for sheet_name, df in excel_file.items():
    print(f"æ­£åœ¨å¤„ç†å·¥ä½œè¡¨: {sheet_name}")
    
    # === ä¸ºå½“å‰å·¥ä½œè¡¨åˆ›å»ºä¸“å±ç›®å½• ===
    safe_sheet_name = safe_filename(sheet_name)
    sheet_output_dir = os.path.join(MAIN_OUTPUT_DIR, safe_sheet_name)
    os.makedirs(sheet_output_dir, exist_ok=True)
    print(f"  ğŸ“ åˆ›å»º/ä½¿ç”¨ç›®å½•: {sheet_output_dir}")
    
    # æ£€æŸ¥å¿…è¦åˆ—æ˜¯å¦å­˜åœ¨
    missing_cols = [col for col in feature_columns + ["age"] if col not in df.columns]
    if missing_cols:
        with open(output_file_path, 'a', encoding='utf-8') as f:
            f.write(f"\nâš ï¸ å·¥ä½œè¡¨ '{sheet_name}' ç¼ºå°‘åˆ—: {missing_cols}ï¼Œè·³è¿‡åˆ†æã€‚\n")
        continue

    # æå–ç‰¹å¾å’Œç›®æ ‡å˜é‡
    X = df[feature_columns]
    y = df["age"]

    # æ£€æŸ¥ç©ºå€¼
    if X.isnull().any().any() or y.isnull().any():
        with open(output_file_path, 'a', encoding='utf-8') as f:
            f.write(f"\nâš ï¸ å·¥ä½œè¡¨ '{sheet_name}' åŒ…å«ç¼ºå¤±å€¼ï¼Œè·³è¿‡åˆ†æã€‚\n")
        continue

    # åˆ’åˆ†è®­ç»ƒé›†å’Œæµ‹è¯•é›†
    X_train, X_test, y_train, y_test = train_test_split(
        X, y, test_size=0.2, random_state=42
    )

    # === ä¼˜åŒ–å‚æ•°ç½‘æ ¼å‡å°‘è¿‡æ‹Ÿåˆ ===
    param_grid = {
        'n_estimators': [100, 200, 300],  # é™åˆ¶æœ€å¤§æ ‘æ•°é‡
        'max_depth': [2, 3, 4],           # ä¸¥æ ¼é™åˆ¶æ ‘æ·±åº¦ï¼ˆå°æ ·æœ¬é˜²è¿‡æ‹Ÿåˆï¼‰
        'learning_rate': [0.01, 0.05, 0.1]  # é™ä½å­¦ä¹ ç‡ï¼Œå¢å¼ºæ³›åŒ–
    }

    # === å¢å¼ºæ­£åˆ™åŒ–å‚æ•° ===
    xgb_model = XGBRegressor(
        random_state=42,
        subsample=0.7,              # å‡å°‘æ ·æœ¬é‡‡æ ·æ¯”ä¾‹
        colsample_bytree=0.7,       # å‡å°‘ç‰¹å¾é‡‡æ ·æ¯”ä¾‹
        reg_alpha=0.3,              # å¢åŠ L1æ­£åˆ™åŒ–
        reg_lambda=1.5,             # å¢åŠ L2æ­£åˆ™åŒ–
        gamma=0.2,                  # å¢åŠ åˆ†è£‚æ‰€éœ€æœ€å°æŸå¤±å‡å°‘
        n_jobs=1,                   # é¿å…å¤šè¿›ç¨‹é—®é¢˜
        tree_method='hist'          # ä½¿ç”¨ç›´æ–¹å›¾ç®—æ³•åŠ é€Ÿ
    )

    # ç½‘æ ¼æœç´¢
    model = GridSearchCV(
        estimator=xgb_model,
        param_grid=param_grid,
        cv=5,
        scoring='neg_mean_absolute_error',
        n_jobs=1,  # å•è¿›ç¨‹æ›´ç¨³å®š
        verbose=0
    )

    # è®­ç»ƒæ¨¡å‹
    model.fit(X_train, y_train)
    best_mae = -model.best_score_
    best_params = model.best_params_

    # é¢„æµ‹
    y_pred = model.predict(X_test)
    y_train_pred = model.predict(X_train)  # è®­ç»ƒé›†é¢„æµ‹

    # === è®¡ç®—å››ä¸ªæ ¸å¿ƒæŒ‡æ ‡ ===
    train_r2 = r2_score(y_train, y_train_pred)
    test_r2 = r2_score(y_test, y_pred)
    train_mae = mean_absolute_error(y_train, y_train_pred)
    test_mae = mean_absolute_error(y_test, y_pred)

    # === ç”Ÿæˆå¯è§†åŒ–å›¾è¡¨ ===
    print(f"  ğŸ“ˆ æ­£åœ¨ç”Ÿæˆå¯è§†åŒ–å›¾è¡¨åˆ°: {sheet_output_dir}")
    
    # 1. ç‰¹å¾é‡è¦æ€§æ¡å½¢å›¾
    plot_feature_importance(model.best_estimator_, feature_columns, sheet_name, sheet_output_dir)
    
    # 2. è¯¯å·®åˆ†å¸ƒå›¾ (ä½¿ç”¨æµ‹è¯•é›†)
    plot_error_distribution(y_test, y_pred, sheet_name, sheet_output_dir)
    
    # 3. SHAPåˆ†æ (ä½¿ç”¨è®­ç»ƒé›†çš„å­é›†åŠ é€Ÿè®¡ç®—)
    sample_size = min(1000, len(X_train))  # é™åˆ¶SHAPè®¡ç®—çš„æ ·æœ¬é‡
    X_sample = X_train.sample(n=sample_size, random_state=42)
    plot_shap_summary(model.best_estimator_, X_sample, feature_columns, sheet_name, sheet_output_dir)
    
    print(f"  âœ… å¯è§†åŒ–å›¾è¡¨ç”Ÿæˆå®Œæˆ")
    print(f"    - ç‰¹å¾é‡è¦æ€§å›¾: ç‰¹å¾é‡è¦æ€§_{safe_sheet_name}.png")
    print(f"    - è¯¯å·®åˆ†å¸ƒå›¾: è¯¯å·®åˆ†å¸ƒ_{safe_sheet_name}.png")
    print(f"    - SHAPæ‘˜è¦å›¾: SHAPæ‘˜è¦_{safe_sheet_name}.png")
    print(f"    - SHAPèœ‚ç¾¤å›¾: SHAPèœ‚ç¾¤å›¾_{safe_sheet_name}.png")

    # === ä¿å­˜æœ¬å·¥ä½œè¡¨çš„å®Œæ•´ç»“æœåˆ°æ—¥å¿—æ–‡ä»¶ ===
    with open(output_file_path, 'a', encoding='utf-8') as f:
        f.write(f"\n{'='*50}\n")
        f.write(f"å·¥ä½œè¡¨ '{sheet_name}' çš„å®Œæ•´åˆ†æç»“æœ\n")
        f.write(f"{'='*50}\n")
        
        f.write(f"å›¾è¡¨ä¿å­˜ç›®å½•: {sheet_output_dir}\n")
        f.write("-" * 30 + "\n")

        f.write("æ¨¡å‹é…ç½®ä¿¡æ¯:\n")
        f.write("-" * 30 + "\n")
        f.write(f"æœ€ä½³å‚æ•°: {best_params}\n")
        f.write(f"å‚æ•°ç½‘æ ¼: {param_grid}\n")
        f.write("å¢å¼ºæ­£åˆ™åŒ–é…ç½®:\n")
        f.write("  subsample=0.7, colsample_bytree=0.7\n")
        f.write("  reg_alpha=0.3, reg_lambda=1.5, gamma=0.2\n")
        f.write(f"äº¤å‰éªŒè¯æŠ˜æ•°: 5\n")

        f.write("\næ¨¡å‹æ€§èƒ½æŒ‡æ ‡:\n")
        f.write("-" * 30 + "\n")
        # === æ·»åŠ å››ä¸ªæ ¸å¿ƒæŒ‡æ ‡ ===
        f.write(f"è®­ç»ƒé›† RÂ²: {train_r2:.4f}\n")
        f.write(f"æµ‹è¯•é›† RÂ²: {test_r2:.4f}\n")
        f.write(f"è®­ç»ƒé›† MAE: {train_mae:.2f} å²\n")
        f.write(f"æµ‹è¯•é›† MAE: {test_mae:.2f} å²\n")
        f.write(f"æœ€ä½³äº¤å‰éªŒè¯ MAE: {best_mae:.2f}\n")

        f.write("\nç‰¹å¾é‡è¦æ€§æ’åº:\n")
        f.write("-" * 30 + "\n")
        importances = model.best_estimator_.feature_importances_
        feature_importance = list(zip(feature_columns, importances))
        feature_importance.sort(key=lambda x: x[1], reverse=True)
        for name, importance in feature_importance:
            f.write(f"{name}: {importance:.4f}\n")

        f.write("\næ•°æ®é›†ä¿¡æ¯:\n")
        f.write("-" * 30 + "\n")
        f.write(f"æ€»æ ·æœ¬æ•°: {len(X)} (å°æ ·æœ¬æ³¨æ„è¿‡æ‹Ÿåˆé£é™©)\n")
        f.write(f"è®­ç»ƒé›†æ ·æœ¬æ•°: {len(X_train)}\n")
        f.write(f"æµ‹è¯•é›†æ ·æœ¬æ•°: {len(X_test)}\n")
        f.write(f"ç‰¹å¾æ•°é‡: {len(feature_columns)}\n")

        f.write("\né¢„æµ‹ç»“æœå¯¹æ¯” (å‰10ä¸ªæµ‹è¯•æ ·æœ¬):\n")
        f.write("-" * 30 + "\n")
        f.write("çœŸå®å¹´é¾„\té¢„æµ‹å¹´é¾„\tè¯¯å·®\n")
        for i in range(min(10, len(y_test))):
            true_age = y_test.iloc[i]
            pred_age = y_pred[i]
            error = abs(true_age - pred_age)
            f.write(f"{true_age:.1f}\t\t{pred_age:.1f}\t\t{error:.1f}\n")

        f.write("\nä½¿ç”¨çš„ç‰¹å¾åˆ—:\n")
        f.write("-" * 30 + "\n")
        for i, col in enumerate(feature_columns, 1):
            f.write(f"{i}. {col}\n")

        f.write(f"\nç”Ÿæˆå›¾è¡¨ (ä¿å­˜åœ¨ {safe_sheet_name} ç›®å½•):\n")
        f.write(f"- ç‰¹å¾é‡è¦æ€§å›¾: ç‰¹å¾é‡è¦æ€§_{safe_sheet_name}.png\n")
        f.write(f"- è¯¯å·®åˆ†å¸ƒå›¾: è¯¯å·®åˆ†å¸ƒ_{safe_sheet_name}.png\n")
        f.write(f"- SHAPæ‘˜è¦å›¾: SHAPæ‘˜è¦_{safe_sheet_name}.png\n")
        f.write(f"- SHAPèœ‚ç¾¤å›¾: SHAPèœ‚ç¾¤å›¾_{safe_sheet_name}.png\n")
        f.write(f"{'='*50}\n")

    print(f"âœ… å·¥ä½œè¡¨ '{sheet_name}' åˆ†æå®Œæˆï¼Œç»“æœå·²è¿½åŠ åˆ°æ—¥å¿—æ–‡ä»¶ã€‚")

print(f"\næ‰€æœ‰å·¥ä½œè¡¨åˆ†æå®Œæ¯•ï¼è¯¦ç»†ç»“æœå·²è¿½åŠ åˆ°: {output_file_path}")