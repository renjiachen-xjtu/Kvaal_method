# Êñá‰ª∂ÂêçÂª∫ËÆÆ‰øùÂ≠ò‰∏∫Ôºöage_prediction.py
# ÂäüËÉΩÔºöËØªÂèñ Excel Â§ö‰∏™Â∑•‰ΩúË°®ÔºåÁî® M-V ÂàóÈ¢ÑÊµã C ÂàóÔºàÂπ¥ÈæÑÔºâÔºåÈöèÊú∫Ê£ÆÊûóÂõûÂΩí + Êñ∞Â¢ûÂèØËßÜÂåñ

import pandas as pd
from sklearn.ensemble import RandomForestRegressor
from sklearn.metrics import r2_score, mean_absolute_error
from sklearn.model_selection import train_test_split, KFold
import numpy as np
import logging
from pathlib import Path
import matplotlib.pyplot as plt
import seaborn as sns
import shap
import os
import warnings
warnings.filterwarnings('ignore')  # ÂøΩÁï•ÊâÄÊúâË≠¶Âëä

# === ÈÖçÁΩÆÂå∫ÔºöËØ∑Á°Æ‰øùË∑ØÂæÑÊ≠£Á°Æ ==
EXCEL_PATH = r"D:\Users\HUAWEI\Desktop\FS\female.xlsx"
OUTPUT_DIR = r"D:\Users\HUAWEI\Desktop\FS"  # Êñ∞Â¢ûÂõæË°®ËæìÂá∫ÁõÆÂΩï
os.makedirs(OUTPUT_DIR, exist_ok=True)  # Á°Æ‰øùËæìÂá∫ÁõÆÂΩïÂ≠òÂú®

# ==============================
# ÈÖçÁΩÆÊó•Âøó
log_file = r"D:\Users\HUAWEI\Desktop\FS\femaletest.txt"

logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(levelname)s - %(message)s',
    handlers=[
        logging.FileHandler(log_file, encoding='utf-8'),
        logging.StreamHandler()  # ÂêåÊó∂ËæìÂá∫Âà∞ÊéßÂà∂Âè∞
    ]
)

def log_and_print(message):
    """ÂêåÊó∂ÊâìÂç∞Âà∞ÊéßÂà∂Âè∞ÂíåËÆ∞ÂΩïÂà∞Êó•Âøó"""
    print(message)
    logging.info(message)

def safe_filename(name):
    """ÁßªÈô§Êñá‰ª∂Âêç‰∏≠ÁöÑÈùûÊ≥ïÂ≠óÁ¨¶"""
    return "".join(c if c.isalnum() or c in (' ', '-', '_') else '_' for c in str(name))

def plot_feature_importance(model, feature_names, sheet_name, output_dir):
    """ÁªòÂà∂ÁâπÂæÅÈáçË¶ÅÊÄßÊù°ÂΩ¢Âõæ"""
    importances = model.feature_importances_
    indices = np.argsort(importances)[::-1]
    
    plt.figure(figsize=(10, 6))
    plt.title(f"ÁâπÂæÅÈáçË¶ÅÊÄß - {sheet_name}", fontsize=14)
    plt.bar(range(len(importances)), importances[indices], color="b", align="center")
    plt.xticks(range(len(importances)), [feature_names[i] for i in indices], rotation=45)
    plt.xlim([-1, len(importances)])
    plt.tight_layout()
    safe_name = safe_filename(sheet_name)
    plt.savefig(os.path.join(output_dir, f"ÁâπÂæÅÈáçË¶ÅÊÄß_{safe_name}.png"), dpi=300)
    plt.close()

def plot_error_distribution(y_true, y_pred, sheet_name, output_dir):
    """ÁªòÂà∂È¢ÑÊµãËØØÂ∑ÆÂàÜÂ∏ÉÂõæ"""
    errors = y_true - y_pred
    
    plt.figure(figsize=(10, 6))
    sns.histplot(errors, kde=True, bins=20, color='skyblue', edgecolor='black')
    plt.axvline(x=np.mean(errors), color='r', linestyle='--', label=f'Âπ≥ÂùáËØØÂ∑Æ: {np.mean(errors):.2f}')
    plt.xlabel("È¢ÑÊµãËØØÂ∑Æ (ÂÆûÈôÖÂπ¥ÈæÑ - È¢ÑÊµãÂπ¥ÈæÑ)", fontsize=12)
    plt.ylabel("Ê†∑Êú¨Êï∞Èáè", fontsize=12)
    plt.title(f"È¢ÑÊµãËØØÂ∑ÆÂàÜÂ∏É - {sheet_name}", fontsize=14)
    plt.legend()
    plt.grid(alpha=0.3)
    safe_name = safe_filename(sheet_name)
    plt.savefig(os.path.join(output_dir, f"ËØØÂ∑ÆÂàÜÂ∏É_{safe_name}.png"), dpi=300)
    plt.close()

def plot_shap_summary(model, X_sample, feature_names, sheet_name, output_dir):
    """ÁªòÂà∂SHAPÊëòË¶ÅÂõæÂíåËúÇÁæ§Âõæ"""
    # ÂàõÂª∫SHAPËß£ÈáäÂô®
    explainer = shap.TreeExplainer(model)
    shap_values = explainer.shap_values(X_sample)
    
    # SHAPÊëòË¶ÅÂõæ
    plt.figure(figsize=(12, 8))
    shap.summary_plot(shap_values, X_sample, feature_names=feature_names, plot_type="bar", show=False)
    plt.title(f"SHAP ÁâπÂæÅÈáçË¶ÅÊÄßÊëòË¶Å - {sheet_name}", fontsize=14)
    safe_name = safe_filename(sheet_name)
    plt.savefig(os.path.join(output_dir, f"SHAPÊëòË¶Å_{safe_name}.png"), dpi=300, bbox_inches='tight')
    plt.close()
    
    # SHAPËúÇÁæ§Âõæ (Violin Plot)
    plt.figure(figsize=(12, 8))
    shap.summary_plot(shap_values, X_sample, feature_names=feature_names, show=False)
    plt.title(f"SHAP ËúÇÁæ§Âõæ - {sheet_name}", fontsize=14)
    plt.savefig(os.path.join(output_dir, f"SHAPËúÇÁæ§Âõæ_{safe_name}.png"), dpi=300, bbox_inches='tight')
    plt.close()

def main():
    try:
        # ËØªÂèñÊâÄÊúâÂ∑•‰ΩúË°®
        xls = pd.ExcelFile(EXCEL_PATH)
        sheet_names = xls.sheet_names
        log_and_print(f"Ê£ÄÊµãÂà∞ {len(sheet_names)} ‰∏™Â∑•‰ΩúË°®: {sheet_names}\n")

        # ÂÆö‰πâÁâπÂæÅÂêçÁß∞ (MÂà∞VÂàó)
        feature_names = [chr(ord('M') + i) for i in range(10)]  # ÁîüÊàê ['M', 'N', 'O', ..., 'V']
        
        for sheet in sheet_names:
            log_and_print(f"{'='*50}")
            log_and_print(f"Â§ÑÁêÜÂ∑•‰ΩúË°®: {sheet}")
            log_and_print(f"{'='*50}")
            
            # ËØªÂèñÊï∞ÊçÆ
            df = pd.read_excel(EXCEL_PATH, sheet_name=sheet, header=None)
            
            # Ëá™Âä®Ë∑≥ËøáË°®Â§¥Ë°åÔºàÂ¶ÇÊûúÁ¨¨‰∏ÄË°åÂåÖÂê´ 'number' Êàñ 'age'Ôºâ
            if df.shape[0] > 0 and ('number' in str(df.iloc[0, 0]) or 'age' in str(df.iloc[0, 2])):
                df = df.iloc[1:].reset_index(drop=True)
            
            # ËΩ¨‰∏∫Êï∞ÂÄºÂπ∂Ê∏ÖÁêÜ
            df = df.apply(pd.to_numeric, errors='coerce')
            df = df.dropna()
            
            if df.empty or len(df) < 5:
                log_and_print("  ‚ö†Ô∏è Êï∞ÊçÆ‰∏çË∂≥ÊàñÊó†ÊïàÔºåË∑≥Ëøá„ÄÇ\n")
                continue

            # ÊèêÂèñÂπ¥ÈæÑÔºàÁ¨¨3ÂàóÔºåCÂàóÔºåÁ¥¢Âºï2Ôºâ
            y = df.iloc[:, 1]  # Ê≥®ÊÑèÔºöËøôÈáå‰øÆÊ≠£‰∏∫Á¥¢Âºï2ÔºàCÂàóÔºâ
            # ÊèêÂèñÁâπÂæÅÔºàMÂà∞VÂàóÔºöÁ¨¨13~22ÂàóÔºåÁ¥¢Âºï12~21Ôºâ
            X = df.iloc[:, 11:21]  # ‰øÆÊ≠£‰∏∫Á¥¢Âºï12-21ÔºàÂÖ±10ÂàóÔºâ
            X.columns = feature_names  # ËÆæÁΩÆÁâπÂæÅÂêçÁß∞

            if X.shape[1] != 10:
                log_and_print(f"  ‚ö†Ô∏è ÁâπÂæÅÂàóÊï∞ÈáèÈîôËØØÔºàÂ∫î‰∏∫10ÂàóÔºåÂÆûÈôÖ{X.shape[1]}ÔºâÔºåË∑≥Ëøá„ÄÇ\n")
                continue

            # ÊõøÊç¢ÂéüÊúâÁöÑÊï∞ÊçÆÈõÜÂàíÂàÜ‰ª£Á†Å
            # ÂàíÂàÜÊï∞ÊçÆÈõÜ‰∏∫ËÆ≠ÁªÉÈõÜ„ÄÅÈ™åËØÅÈõÜÂíåÊµãËØïÈõÜ
            X_temp, X_test, y_temp, y_test = train_test_split(
                X, y, test_size=0.2, random_state=42
            )
            X_train, X_val, y_train, y_val = train_test_split(
                X_temp, y_temp, test_size=0.25, random_state=42  # 0.25 * 0.8 = 0.2 of original data
            )

            log_and_print(f"  üìä Êï∞ÊçÆÈõÜÂàíÂàÜ: ËÆ≠ÁªÉÈõÜ({len(X_train)}), È™åËØÅÈõÜ({len(X_val)}), ÊµãËØïÈõÜ({len(X_test)})")
            
            # ÊõøÊç¢ÂéüÊúâÁöÑÂèÇÊï∞ÊµãËØïÈÉ®ÂàÜ‰∏∫‰ª•‰∏ã‰ª£Á†Å
            def cross_validate_with_regularization(X, y):
                """‰ΩøÁî®Ê≠£ÂàôÂåñÂèÇÊï∞ËøõË°å‰∫§ÂèâÈ™åËØÅ"""
                
                # ÂèÇÊï∞ÁΩëÊ†ºÔºàÊõ¥Ê≥®ÈáçÈò≤Ê≠¢ËøáÊãüÂêàÔºâ
                param_grid = {
                    'n_estimators': [50, 100, 150],           # Â¢ûÂä†‰∏Ä‰∏™‰∏≠Èó¥ÂÄº
                    'max_features': ['sqrt', 'log2', 2, 3, 4], # Ê∑ªÂä†log2ÈÄâÈ°π
                    'max_depth': [3, 5, 8, None],             # Ê∑ªÂä†‰∏çÈôêÂà∂Ê∑±Â∫¶ÁöÑÈÄâÈ°π
                    'min_samples_split': [10, 15, 20, 25],    # Â¢ûÂä†‰∏Ä‰∏™Êõ¥Â§ßÂÄº
                    'min_samples_leaf': [5, 8, 10, 15],       # Â¢ûÂä†‰∏Ä‰∏™Êõ¥Â§ßÂÄº
                   'bootstrap': [True, False],               # Ê∑ªÂä†ÊòØÂê¶ÊîæÂõûÊäΩÊ†∑ÈÄâÈ°π
                    'max_samples': [0.6, 0.8, 1.0]           # Ê∑ªÂä†ÈááÊ†∑ÊØî‰æãÔºà‰ªÖÂΩìbootstrap=TrueÊó∂ÊúâÊïàÔºâ
                 }
                
                best_mae = np.inf
                best_params = {}
                best_r2 = -np.inf
                
                log_and_print(f"  üìã ÊµãËØïÂèÇÊï∞ÁªÑÂêà...")
                
                # ÁÆÄÂåñÁâàÁΩëÊ†ºÊêúÁ¥¢
                for n_est in param_grid['n_estimators'][:2]:  # Âè™ÂèñÂâç‰∏§‰∏™ÂÄº
                    for max_feat in param_grid['max_features']:
                        for max_dep in param_grid['max_depth'][:2]:  # Âè™ÂèñÂâç‰∏§‰∏™ÂÄº
                            for min_ss in param_grid['min_samples_split']:
                                for min_sl in param_grid['min_samples_leaf']:
                                    
                                    kfold = KFold(n_splits=5, shuffle=True, random_state=42)
                                    fold_mae_scores = []
                                    fold_r2_scores = []
                                    
                                    # ËøõË°å5Êäò‰∫§ÂèâÈ™åËØÅ
                                    for train_index, val_index in kfold.split(X):
                                        X_train_fold, X_val_fold = X.iloc[train_index], X.iloc[val_index]
                                        y_train_fold, y_val_fold = y.iloc[train_index], y.iloc[val_index]
                                        
                                        # ÂàõÂª∫Ê®°Âûã
                                        model = RandomForestRegressor(
                                            n_estimators=n_est,
                                            max_features=max_feat,
                                            max_depth=max_dep,
                                            min_samples_split=min_ss,
                                            min_samples_leaf=min_sl,
                                            random_state=42,
                                            n_jobs=1
                                        )
                                        
                                        model.fit(X_train_fold, y_train_fold)
                                        
                                        # È¢ÑÊµã‰∏éËØÑ‰º∞
                                        y_pred = model.predict(X_val_fold)
                                        fold_mae_scores.append(mean_absolute_error(y_val_fold, y_pred))
                                        fold_r2_scores.append(r2_score(y_val_fold, y_pred))
                                    
                                    # ËÆ°ÁÆóÂπ≥ÂùáÂàÜÊï∞
                                    avg_mae = np.mean(fold_mae_scores)
                                    avg_r2 = np.mean(fold_r2_scores)
                                    
                                    # Êõ¥Êñ∞ÊúÄ‰Ω≥ÂèÇÊï∞Ôºà‰ª•MAE‰∏∫ÂáÜÔºâ
                                    if avg_mae < best_mae:
                                        best_mae = avg_mae
                                        best_r2 = avg_r2
                                        best_params = {
                                            'n_estimators': n_est,
                                            'max_features': max_feat,
                                            'max_depth': max_dep,
                                            'min_samples_split': min_ss,
                                            'min_samples_leaf': min_sl
                                        }
                
                return best_params, best_r2, best_mae

            # Âú®‰∏ªÂæ™ÁéØ‰∏≠‰ΩøÁî®Ëøô‰∏™ÂáΩÊï∞
            log_and_print("  üîç ÂºÄÂßãÂèÇÊï∞ÊêúÁ¥¢ÔºàÈò≤Ê≠¢ËøáÊãüÂêà‰ºòÂåñÔºâ...")
            best_params, best_r2, best_mae = cross_validate_with_regularization(X, y)

            log_and_print(f"\n  üèÜ ÊúÄ‰Ω≥ÂèÇÊï∞ÁªÑÂêà:")
            for key, value in best_params.items():
                log_and_print(f"    {key}: {value}")
            log_and_print(f"    ÊúÄ‰Ω≥ R¬≤ Score: {best_r2:.4f}")
            log_and_print(f"    ÂØπÂ∫î MAE: {best_mae:.2f} Â≤Å")

            # ‰ΩøÁî®ÊúÄ‰Ω≥ÂèÇÊï∞ÈáçÊñ∞ËÆ≠ÁªÉÊ®°ÂûãÔºà‰øÆÊîπËøôÈÉ®ÂàÜÔºâ
            best_model = RandomForestRegressor(
                n_estimators=best_params['n_estimators'],
                max_features=best_params['max_features'],
                max_depth=best_params.get('max_depth', 10),  # ÈôêÂà∂Ê†ëÁöÑÊ∑±Â∫¶
                min_samples_split=best_params.get('min_samples_split', 10),  # Â¢ûÂä†ÂàÜË£ÇÊâÄÈúÄÊúÄÂ∞èÊ†∑Êú¨Êï∞
                min_samples_leaf=best_params.get('min_samples_leaf', 4),  # Â¢ûÂä†Âè∂ËäÇÁÇπÊúÄÂ∞èÊ†∑Êú¨Êï∞
                random_state=42,
                n_jobs=-1
            )
            best_model.fit(X_train, y_train)
            
            # ======== Êñ∞Â¢ûÔºöËÆ°ÁÆóËÆ≠ÁªÉÈõÜÂíåÊµãËØïÈõÜR¬≤ ========
            y_train_pred = best_model.predict(X_train)
            y_test_pred = best_model.predict(X_test)
            
            train_r2 = r2_score(y_train, y_train_pred)
            test_r2 = r2_score(y_test, y_test_pred)
            test_mae = mean_absolute_error(y_test, y_test_pred)
            
            log_and_print(f"  ‚úÖ Ê†∑Êú¨ÊÄªÊï∞: {len(df)}")
            log_and_print(f"  üìä ËÆ≠ÁªÉÈõÜ R¬≤ Score: {train_r2:.4f}")
            log_and_print(f"  üìä ÊµãËØïÈõÜ R¬≤ Score: {test_r2:.4f}")
            log_and_print(f"  üìè ÊµãËØïÈõÜ MAE (Âπ≥ÂùáÁªùÂØπËØØÂ∑Æ): {test_mae:.2f} Â≤Å\n")
            
            # ======== Êñ∞Â¢ûÔºöÁîüÊàêÂèØËßÜÂåñÂõæË°® ========
            safe_sheet_name = safe_filename(sheet)
            sheet_output_dir = os.path.join(OUTPUT_DIR, safe_sheet_name)
            os.makedirs(sheet_output_dir, exist_ok=True)
            
            log_and_print(f"  üìà Ê≠£Âú®ÁîüÊàêÂèØËßÜÂåñÂõæË°®Âà∞: {sheet_output_dir}")
            
            # 1. ÁâπÂæÅÈáçË¶ÅÊÄßÂõæ
            plot_feature_importance(best_model, feature_names, sheet, sheet_output_dir)
            
            # 2. ËØØÂ∑ÆÂàÜÂ∏ÉÂõæ (‰ΩøÁî®ÊµãËØïÈõÜ)
            plot_error_distribution(y_test, y_test_pred, sheet, sheet_output_dir)
            
            # 3. SHAPÂàÜÊûê (‰ΩøÁî®ËÆ≠ÁªÉÈõÜÁöÑÂ≠êÈõÜÂä†ÈÄüËÆ°ÁÆó)
            sample_size = min(1000, len(X_train))  # ÈôêÂà∂SHAPËÆ°ÁÆóÁöÑÊ†∑Êú¨Èáè
            X_sample = X_train.sample(n=sample_size, random_state=42)
            plot_shap_summary(best_model, X_sample, feature_names, sheet, sheet_output_dir)
            
            log_and_print(f"  ‚úÖ ÂèØËßÜÂåñÂõæË°®ÁîüÊàêÂÆåÊàê")
            log_and_print(f"    - ÁâπÂæÅÈáçË¶ÅÊÄßÂõæ: ÁâπÂæÅÈáçË¶ÅÊÄß_{safe_sheet_name}.png")
            log_and_print(f"    - ËØØÂ∑ÆÂàÜÂ∏ÉÂõæ: ËØØÂ∑ÆÂàÜÂ∏É_{safe_sheet_name}.png")
            log_and_print(f"    - SHAPÊëòË¶ÅÂõæ: SHAPÊëòË¶Å_{safe_sheet_name}.png")
            log_and_print(f"    - SHAPËúÇÁæ§Âõæ: SHAPËúÇÁæ§Âõæ_{safe_sheet_name}.png\n")

    except FileNotFoundError:
        log_and_print(f"‚ùå ÈîôËØØÔºöÊâæ‰∏çÂà∞Êñá‰ª∂ÔºÅËØ∑Ê£ÄÊü•Ë∑ØÂæÑÊòØÂê¶Ê≠£Á°ÆÔºö\n{EXCEL_PATH}")
    except Exception as e:
        log_and_print(f"üí• Á®ãÂ∫èÂá∫ÈîôÔºö{e}")
        import traceback
        traceback.print_exc()

if __name__ == "__main__":
    main()
    log_and_print("‚úÖ ÊâÄÊúâÂ∑•‰ΩúË°®Â§ÑÁêÜÂÆåÊØïÔºÅ")